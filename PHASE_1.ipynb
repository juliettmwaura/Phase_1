{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is stored in data structures that are built into the Python language: numbers, strings, lists, dictionaries, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_table = [\n",
    "    {\"color\": \"green\", \"number\": 7},\n",
    "    {\"color\": \"red\", \"number\": 2},\n",
    "    {\"color\": \"orange\", \"number\": 1}\n",
    "]\n",
    "\n",
    "type(info_table) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "green\n",
      "red\n",
      "orange\n"
     ]
    }
   ],
   "source": [
    "for row in info_table:\n",
    "    print(row[\"color\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(info_table)\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     green\n",
      "1       red\n",
      "2    orange\n",
      "Name: color, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df[\"color\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Modules\n",
    "The main modules we will focus on are csv and json.\n",
    "The first major file type we will explore is CSV (comma-separated value).\n",
    "The second major file type we will explore is JSON (JavaScript object notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy\n",
    "In Python, the most fundamental package used for scientific computation is NumPy (Numerical Python). It provides lots of useful functionality for mathematical operations on vectors and matrices in Python. Matrix computation is the primary strength of NumPy.\n",
    "\n",
    "# SciPy\n",
    "In the Data Science domain, Python’s SciPy stack (a collection of software specifically designed for scientific computing) is used heavily for conducting scientific experiments. SciPy is a library of software for engineering and science applications and contains functions for linear algebra, optimization, integration, and statistics.\n",
    "\n",
    "# Statsmodels\n",
    "Statsmodels is a library for Python that enables its users to conduct data exploration via the use of various methods of estimation of statistical models and performing statistical assertions and analysis. The library provides insights when diagnosing issues with linear regression models, generalized linear models, discrete choice models, robust linear models, and time series analysis models with various estimators.\n",
    "\n",
    "# Pandas\n",
    "Pandas is a Python package designed to work with “relational” data and helps replicates the functionality of relational databases in a simple and intuitive way. It is designed for quick and easy data cleansing, manipulation, aggregation, and visualization.\n",
    "There are two main data structures in the library:\n",
    "\n",
    "“Series” - one-dimensional\n",
    "“DataFrames” - two-dimensional\n",
    " \n",
    "Here are a few ways in which Pandas may come in handy:\n",
    "\n",
    "Easily delete and add columns from DataFrame\n",
    "Convert data structures to DataFrame objects\n",
    "Handle missing data and outliers\n",
    "Powerful grouping and aggregation functionality\n",
    "Offers visualization functionality to plot complex statistical visualizations on the go\n",
    "The data structures in Pandas are highly compatible with most of the other libraries\n",
    "\n",
    "# MatplotLib\n",
    "Matplotlib is another SciPy stack package and a library that is tailored for the generation of simple and powerful visualizations. Line plots\n",
    "Scatter plots\n",
    "Bar charts and Histograms\n",
    "Pie charts\n",
    "Stem plots\n",
    "Contour plots\n",
    "Quiver plots\n",
    "Spectrograms\n",
    "\n",
    "# Seaborn \n",
    "Seaborn extends the functionality of Matplotlib and that’s why it can address the two biggest issues with Matplotlib - the quality of plots and parameter defaults. Your plots with Seaborn will be more attractive, need less time to create, and will reveal more information.\n",
    "\n",
    "# Scikit-Learn\n",
    "For machine learning, one of the most heavily used packages is scikit-learn. The package makes heavy use of its mathematical operations to model and test complex computational algorithms.\n",
    "\n",
    "# Deep Learning  (Keras / TensorFlow)\n",
    "TensorFlow is an open-source library of data flow graph computations, which are fine-tuned for heavy duty Machine Learning. TensorFlow was designed to meet the performance requirements of Google for training Deep Neural Networks in order to analyze visual and textual data. The key feature of TensorFlow is its multi-layered nodes system that enables quick training of artificial neural networks on big data. This is the library that powers Google’s voice recognition and object recognition in real time.\n",
    "Keras is an open-source library for building Neural Networks with a high-level of interface abstraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Methods in Pandas\n",
    "\n",
    "df.info(), .describe(), .mean(), .quantile(),\n",
    ".mode() -- the mode of the column\n",
    ".count() -- the count of the total number of entries in a column\n",
    ".std() -- the standard deviation for the column\n",
    ".var() -- the variance for the column\n",
    ".sum() -- the sum of all values in the column\n",
    ".cumsum() -- the cumulative sum, where each cell index contains the sum of all indices lower than, and including, itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Statistics for Categorical Columns\n",
    "\n",
    "These methods are extremely useful when dealing with categorical data!\n",
    "\n",
    ".unique() shows us all the unique values contained in the column.\n",
    "\n",
    ".value_counts() shows us a count for how many times each unique value is present in a dataset, giving us a feel for the distribution of values in the column.\n",
    "\n",
    "Sometimes, we'll need to make changes to our dataset, or to compute functions on our data that aren't built-in to Pandas. We can do this by passing lambda values into the apply() method when working with Pandas series, and the .applymap() method when working with Pandas DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panda Dataframe\n",
    " \n",
    " Students will learn how to create a dataframe, how to view dataframe contents as well as many pandas methods to help analyze and modify data within a dataframe. Topics covered in this lesson include: read_csv, .index, .info, .describe, .dtypes, .values, .head, .tail, .shape, scatterplot, distplot, .concat, filtering, adding columns, aggregating methods including .mean, .min, .max, .value_counts, and sort_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply lambda functions\n",
    "df['Review_Word_Length'] = df['text'].map(lambda x: len(x.split()))\n",
    "df.head()\n",
    "df.shape\n",
    "# Group data\n",
    "df.groupby('business_id')['stars'].mean().head()\n",
    "# Check for duplicates\n",
    "df.duplicated().value_counts()\n",
    "#Use keep=False to keep all duplicates and sort_values to put duplicates next to each other\n",
    "df[df.duplicated(keep=False)].sort_values(by='business_id')\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "df.shape\n",
    "# Recheck for duplicates\n",
    "df.duplicated().value_counts()\n",
    "#Duplicates should no longer exist\n",
    "df[df.duplicated(keep=False)].sort_values(by='business_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting missing data\n",
    "df.isna()\n",
    "df.isna().sum()\n",
    "\n",
    "# Categorical data\n",
    "df['Embarked'].unique()\n",
    "\n",
    "# Strategies for dealing with missing data\n",
    "We have three options for dealing with missing values -- removing them from the dataset, keeping them, or replacing them with another value.\n",
    "## replacing continous data\n",
    "df['Fare'].fillna(df['Fare'].median())\n",
    "\n",
    " Beginning with an exploratory data analysis (EDA) the data is inspected and pandas packages are used to begin cleaning the data for analysis. Topics included in this lesson are: .info, .describe, .value_counts, .map, .apply, .isna, .unique, lambda functions, handling missing data, dropping missing data, .applymap, and faster numpy methods including np.where and np.select\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using .groupby()\n",
    "df.groupby('Sex').sum()\n",
    "\n",
    "Some of the most common aggregate methods you may want to use are:\n",
    "\n",
    ".min(): returns the minimum value for each column by group\n",
    ".max(): returns the maximum value for each column by group\n",
    ".mean(): returns the average value for each column by group\n",
    ".median(): returns the median value for each column by group\n",
    ".count(): returns the count of each column by group\n",
    "\n",
    "# Multiple groups\n",
    "df.groupby(['Sex', 'Pclass']).mean()\n",
    "\n",
    "# Selecting information from grouped objects\n",
    "df.groupby(['Sex', 'Pclass'])['Survived'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining DataFrames With Pandas\n",
    "to_concat = [df1, df2, df3]\n",
    "big_df = pd.concat(to_concat)\n",
    "\n",
    "some_dataframe.set_index('name_of_index_column', inplace=True)\n",
    "\n",
    "If inplace is not specified it will default to False, meaning that a copy of the DataFrame with the requested changes will be returned, but the original object will remain unchanged.\n",
    " \n",
    "An Outer Join returns all records from both tables\n",
    "An Inner Join returns only the records with matching keys in both tables\n",
    "A Left Join returns all the records from the left table, as well as any records from the right table that have a matching key with a record from the left table\n",
    "A Right Join returns all the records from the right table, as well as any records from the left table that have a matching key with a record from the right table\n",
    "\n",
    "joined_df = df1.join(df2, how='inner')\n",
    "\n",
    "If how= is not specified, it defaults to 'left'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivot tables\n",
    " method creates pivot tables with pandas   .pivot()\n",
    " \n",
    " When I want to combine multiple data frames that have columns in common         .merge()\n",
    " \n",
    "# Lambda functions\n",
    "They're very useful for transforming a column feature. For example, you might want to extract the day from a date.\n",
    "\n",
    "import pandas as pd\n",
    "dates = pd.Series(['12-01-2017', '12-02-2017', '12-03-2017', '12-04-2017'])\n",
    "dates.map(lambda x: x.split('-')[1])\n",
    "\n",
    "# Combining DataFrames\n",
    "You can combine dataframes by merging them (joining data by a common field) or concatenating them (appending data at the beginning or end).\n",
    "\n",
    "df1 = pd.DataFrame(dates)\n",
    "df2 = pd.DataFrame(['12-05-2017', '12-06-2017', '12-07-2017'])\n",
    "pd.concat([df1, df2])\n",
    "\n",
    "# Grouping and aggregating\n",
    "df = pd.read_csv('titanic.csv')\n",
    "df.head()\n",
    "grouped = df.groupby(['Pclass', 'Sex'])['Age'].mean().reset_index()\n",
    "grouped.head()\n",
    "\n",
    "# Pivot tables\n",
    "pivoted = grouped.pivot(index='Pclass', columns = 'Sex', values='Age')\n",
    "pivoted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Structured Query Language\n",
    "\n",
    "SQL databases are containers that can contain multiple tables. Each table has a schema that describes its columns (and their data types), and an entity relationship diagram (or ERD) is a visual representation of all tables and their relationships.\n",
    "\n",
    "Unlike a CSV, a SQL table can also enforce the data types of the columns, which are described in the table schema. The schema for this table might look something like this:\n",
    "\n",
    "CREATE TABLE people (\n",
    "  id INTEGER PRIMARY KEY,\n",
    "  name TEXT,\n",
    "  age INTEGER,\n",
    "  email TEXT\n",
    ");\n",
    "\n",
    "A simple query would look something like this:\n",
    "\n",
    "SELECT col1, col2, col3\n",
    "FROM table\n",
    "WHERE records match criteria\n",
    "LIMIT 100;\n",
    "Don't worry if this is confusing now, you'll soon learn what each line does. For now, just notice that:\n",
    "\n",
    "Queries start with the SELECT clause, followed by what you want to select. If selecting multiple columns, you separate them with a comma.\n",
    "Then you specify where that data is being retrieved from the using the FROM clause followed by the table name.\n",
    "Afterward, you can provide conditions such as filters or limits on the amount of data returned.\n",
    "\n",
    "A primary key is a unique identifier for a table. You'll see that the columns that are the primary key for one table can also appear on other tables. This is known as a foreign key aka the primary key from a different (\"foreign\") table. \n",
    "\n",
    "Using the sqlite3 module, which of the following opens a connection to the database customers?\n",
    "  sqlite3.connect(‘customers’)\n",
    "  \n",
    "After SELECT and FROM, the next SQL clause you're most likely to use as a data scientist is WHERE.\n",
    "\n",
    "With just a SELECT expression, we can specify which columns we want to select, as well as transform the column values using aliases, built-in functions, and other expressions.\n",
    "\n",
    "However if we want to filter the rows that we want to select, we also need to include a WHERE clause.\n",
    "\n",
    "Order the results of your queries by using ORDER BY (ASC & DESC)\n",
    "Limit the number of records returned by a query using LIMIT\n",
    "\n",
    "Describe the relationship between aggregate functions and GROUP BY statements\n",
    "Use GROUP BY statements in SQL to apply aggregate functions like: COUNT, MAX, MIN, and SUM\n",
    "Create an alias in a SQL query\n",
    "Use the HAVING clause to compare different aggregates\n",
    "Compare the difference between the WHERE and HAVING clause\n",
    "\n",
    "For a restaurant with a database table named orders with a numeric column price, which code would return the 3 highest priced orders?     SELECT * FROM orders ORDER BY price DESC LIMIT 3;\n",
    "\n",
    "Which of the following queries is invalid for two tables employees and consumers that each have a person_id column?\n",
    "  SELECT person_id FROM employees, consumers;\n",
    "  \n",
    "write SQL queries that start with SELECT. This allows you to read specific columns by name, or read all columns using SELECT *.\n",
    "\n",
    "If you want to filter your selection so it only contains rows that meet certain criteria, you can use the WHERE clause.\n",
    "\n",
    "If you want to order your selection, you can use the ORDER BY clause. Remember that the default behavior is to return results in ascending order (smallest to largest). You can verbosely specify this with the ASC keyword, or, more commonly, modify the behavior to sort in descending order (largest to smallest) with the DESC keyword.\n",
    "\n",
    "If you want to limit the number of results, you can use the LIMIT clause. This is frequently used with ORDER BY to select the largest or smallest N results.\n",
    "\n",
    "grouping data using the GROUP BY clause.\n",
    "\n",
    "Often you also want to use an aggregate function when grouping, so that you can summarize the grouped data in some meaningful way. Examples of these aggregate functions include COUNT(), MAX(), MIN(), and AVG(). You can also use those aggregate functions without grouping, and they will return a single record for the table overall (or all rows selected based on your filters).\n",
    "\n",
    "If you want to filter based on the result of an aggregate function, you need to use HAVING rather than WHERE. WHERE applies to the original rows of the table, whereas HAVING applies to the rows created by GROUP BY. Both can be used in the same query if needed, applied to different features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL JOIN\n",
    "The SQL JOIN clause is the main way that you will write queries that combine data from multiple tables.\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"payroll.db\")\n",
    "pd.read_sql(\"\"\"SELECT * FROM employees;\"\"\", conn)\n",
    "pd.read_sql(\"\"\"SELECT name FROM managers WHERE id = 1;\"\"\", conn) \n",
    "pd.read_sql(\"\"\"SELECT name FROM managers WHERE id = 2;\"\"\", conn) \n",
    "With a SQL join, we can do it all at once:\n",
    "\n",
    "q = \"\"\"\n",
    "SELECT *\n",
    "FROM employees\n",
    "JOIN managers\n",
    "    ON employees.manager_id = managers.id\n",
    ";\n",
    "\"\"\"\n",
    "pd.read_sql(q, conn)\n",
    "\n",
    "Most of the time when you have a JOIN, you want to specify which columns you actually want, instead of SELECT *. Something like this, using aliases to make everything really clear:\n",
    "\n",
    "q = \"\"\"\n",
    "SELECT\n",
    "    employees.name AS employee_name,\n",
    "    employees.pay AS employee_pay,\n",
    "    managers.name AS manager_name\n",
    "FROM employees\n",
    "JOIN managers\n",
    "    ON employees.manager_id = managers.id\n",
    ";\n",
    "\"\"\"\n",
    "pd.read_sql(q, conn) \n",
    "\n",
    "# SQL Subqueries\n",
    "Another more-advanced technique we will introduce in this section is a SQL subquery. The above query, rewritten to use a subquery instead of JOIN, would be:\n",
    "\n",
    "q = \"\"\"\n",
    "SELECT\n",
    "    name AS employee_name,\n",
    "    pay AS employee_pay,\n",
    "    (\n",
    "        SELECT name\n",
    "        FROM managers\n",
    "        WHERE managers.id = employees.manager_id\n",
    "    ) AS manager_name\n",
    "FROM employees\n",
    ";\n",
    "\"\"\"\n",
    "pd.read_sql(q, conn) \n",
    "\n",
    "conn.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIs - Application Programming Interfaces\n",
    "An API is a communication protocol between 2 software systems. It describes the mechanism through which if one system requests some information using a predefined format, a remote system responds with an outcome that gets sent back to the first system.APIs are a way of allowing 2 applications to interact with each other. \n",
    "\n",
    "An API has three main components as listed below:\n",
    "\n",
    "Access Permissions: Is the user allowed to ask for data or services?\n",
    "Request: The service being asked for (e.g., if I give you current location using GPS, tell me the map around that place - as we see in Pokemon Go). A Request has two main parts:\n",
    "\n",
    "Methods: Once the access is permitted, what questions can be asked.\n",
    "Parameters: Additional details that can be sent with requests or responses\n",
    "Response: The data or service as a result of the request.\n",
    "\n",
    "A  client is a computer hardware device or software that requests a service made available by a server. The server is often (but not always) located on a separate physical computer.\n",
    "A server is a physical computer dedicated to run services to serve the needs of clients. Depending on the service that is running, it could be a file server, database server, home media server, print server, email server or a web server.\n",
    "The idea of a Client and Server communicating over a network is what makes viewing websites and interacting with Web applications (like Gmail, Facebook, LinkedIn) possible. This model is a way to describe the give-and-take relationship between the client and server in a Web application and governs how information passes between computers.\n",
    "A Web application (Web app) is an application program that is stored on a remote server and delivered over the Internet through a browser interface\n",
    "\n",
    "# The Web client\n",
    "The client is what the end user interacts with. \"Client-side\" code is actually responsible for most of what a user actually sees. For requesting some information as a web page, the client side may be responsible for: includes:\n",
    "\n",
    "Defining the structure of the Web page\n",
    "Setting the look and feel of the Web page\n",
    "Implementing a mechanism for responding to user interactions (clicking buttons, entering text, etc.)\n",
    "Most of these tasks are managed by HTML/CSS/JavaScript-like technologies to structure the information, style of the page and provide interactive objects for navigation and focus. \n",
    "\n",
    "# The Web Server\n",
    "A web server in a Web application is what listens to requests coming in from the clients. When you set up an HTTP (HyperText Transfer Protocol - Language of the internet) server, we set it up to listen to a port number. A port number is always associated with the IP address of a computer. You can think of ports as separate channels on a computer that we can use to perform different tasks: one port could be surfing www.facebook.comLinks to an external site. while another fetches your email.\n",
    "\n",
    "# The Database\n",
    "Databases are the foundations of Web architecture. An SQL/NoSQL or a similar type of database is a place to store information so that it can easily be accessed, managed, and updated. If you're building a social media site, for example, you might use a database to store information about your users, posts, comments, etc. When a visitor requests a page, the data inserted into the page comes from the site's database, allowing real-time user interactions with sites like Facebook or apps like Gmail.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OAuth\n",
    "OAuth stands for Open Authorization.\n",
    "\n",
    "OAuth is an open-source protocol created to allow the creators of APIs and other online services to easily let them share private data or assets with users. One of the biggest challenges of building multi-user applications is making sure that you only give people access to the data and functionality they're supposed to have. OAuth provides a framework for allowing authenticated access, but without the risk of having to share the original login credentials such as a password.\n",
    "\n",
    "It allows applications to have limited scopes to user data \n",
    "\n",
    "What does the folium package create? Interactive maps \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
